# network_sigma
Originally just me playing around with using neuronal networks to find a very simple function (random walk probability to go to first to point 100, than to point -100 as function of the starting position), but evolving into an idea of generating sigmas for the output of a neuronal network. This works, by letting the network decide itself what it more certain of, and what it is not (and than using some clever math, aswell as a normalisation)


This is based on the fact, that sum((sigma_i/alpha_i)^2) where sum(alpha_i)=const is minimized at alpha_i\~sigma_i^(2/3). This means, by introducing a learnable division variable into the loss (that the network can decide to give any dynamic value to, see main.py), we can get the sigma of this variable by just applying a power 1.5. Sadly this still needs some kind of normalisation (which makes sense, since the standart deviation of a continuos variable is not so easily defined). We use here (nplot.py) the chi² test to assert that chi²~=1, but the standart deviation for a single point should work well to.

This is restricted to L2 losses (mse), but could be easily extended for LN losses (l1 for example follows alpha_1~sigma_i^(1/2)).


test68 contains an alternative idea: Instead of letting the network use variables to hide what it does not like, this just punishes the standart deviation for not meeting our expectations. This is much more unstable (for me it did not work at all with higher batch sizes, which is strange, since these kind of analyses should improve by higher batch sizes). But when it does work, it reproduces an entirely different shape. While division based networks seem to produce standart deviations that are less  accurate around input=0, these seem to reproduce the opposite behaviour: The ranges seem to be reproduced less accurately.

If you have any idea why this could be, I would love to hear from you (Simon.Kluettermann@rwth-aachen.de), but here are some of my guesses:
    If you look at the distribution, it is also much flatter: For me this could be a result, from sigma not being completely local (the network can define one constant loss, instead of defining a more complicated loss). This means that it just tries to find the most optimal loss for my training data. And even though I would not directly say that this is terrible (defining one standart deviation for a couple of variables is actually really useful, as it makes the network more understandable. Especcially when you consider that the network so decides itself if it should use a constant approximation, or if there is a real difference between the inputs), this means that the standart deviation depends on the distribution in my training data. This migth seem obvious, but is definitely something to keep in mind. Especcially when you have less equal distributed (or ordered) input datapoints, this can lead to serious problems.
    
    Now to think about why the last points are less accurate: When the standart deviation is constant, you expect corrections at the extremes of this approximation. These extremes are the points, at which the highest loss can be achieved and where there is the most randomness involved. Both of these are at the endpoints, as there is the lowest change for the highest possible (random) correction. And since the network only punishes the average, it just needs to reduce variance, and this is done there. And since a less important (higher sigma) value always has a lower variance, this learns to increase the variance at the extremas (aka at the border). This is not what you want, since generally I would assume that the variance is much lower here (at 99 as input, we can surely say that the result should not be 0). For me this means, that you should probably not use test68 code, but if you have a different opinion, I would love to discuss this



